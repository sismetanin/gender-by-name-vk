{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect VK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vk\n",
      "Requirement already satisfied: requests<3.0,>=2.8 in /Users/sismetanin/opt/anaconda3/lib/python3.7/site-packages (from vk) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/sismetanin/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.8->vk) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sismetanin/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.8->vk) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/sismetanin/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.8->vk) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sismetanin/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0,>=2.8->vk) (2019.9.11)\n",
      "Installing collected packages: vk\n",
      "Successfully installed vk-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vk\n",
    "session = vk.Session(access_token='TOKEN')\n",
    "vk_api = vk.API(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ru = []\n",
    "items_en = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, results['count'], 1000)):\n",
    "    results = vk_api.groups.getMembers(group_id=27895931, sort=\"id_desc\", offset=i, count=1000, v=5.81, fields=['sex'], lang='ru')\n",
    "    items_ru.append(results['items'])\n",
    "    results = vk_api.groups.getMembers(group_id=27895931, sort=\"id_desc\",  offset=i, count=1000, v=5.81, fields=['sex'], lang='en')\n",
    "    items_en.append(results['items'])\n",
    "    time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess VK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ru = pd.read_csv('names_ru.csv')\n",
    "df_en = pd.read_csv('names_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ru = df_ru.drop_duplicates(subset=['id'])\n",
    "df_en = df_en.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ru = df_ru.dropna()\n",
    "df_en = df_en.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13131232/13131232 [00:14<00:00, 933523.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "first_names = df_ru['first_name'].values\n",
    "last_names = df_ru['last_name'].values\n",
    "\n",
    "df_ru['full_name'] = [str(first_names[i]).lower().strip() + ' '+last_names[i].lower().strip() for i in tqdm(range(len(df_ru)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13129236/13129236 [00:12<00:00, 1059306.62it/s]\n"
     ]
    }
   ],
   "source": [
    "first_names = df_en['first_name'].values\n",
    "last_names = df_en['last_name'].values\n",
    "\n",
    "df_en['full_name'] = [str(first_names[i]).lower().strip() + ' '+last_names[i].lower().strip() \n",
    "                      for i in tqdm(range(len(df_en)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_ru, df_en, on=['id', 'sex'], how='left', suffixes=['_ru', '_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sex']!=0]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13124619"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "genders = []\n",
    "\n",
    "temp = df[df['full_name_en']!=df['full_name_ru']]\n",
    "names.extend(temp['full_name_en'].values)\n",
    "genders.extend(temp['sex'].values)\n",
    "names.extend(temp['full_name_ru'].values)\n",
    "genders.extend(temp['sex'].values)\n",
    "\n",
    "temp = df[df['full_name_en']==df['full_name_ru']]\n",
    "names.extend(temp['full_name_ru'].values)\n",
    "genders.extend(temp['sex'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25101673"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'name': names, 'sex': genders})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = df_train.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__analyzer': ['char', 'char_wb'], 'vect__max_features': (5000, 10000, 20000), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__min_df': (0.0, 0.01, 0.05), 'vect__ngram_range': [(2, 3), (2, 5), (2, 7)], 'tfidf__use_idf': (True, False), 'tfidf__norm': ['l1', 'l2']}\n",
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1944 out of 1944 | elapsed: 42.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.977\n",
      "Best parameters set:\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: 20000\n",
      "\tvect__min_df: 0.0\n",
      "\tvect__ngram_range: (2, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LogisticRegression(random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"vect__analyzer\": ['char', 'char_wb'],\n",
    "    \"vect__max_features\": (5000, 10000, 20000),\n",
    "    \"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"vect__min_df\": (0.0, 0.01, 0.05),\n",
    "    \"vect__ngram_range\": [(2,3), (2,5),(2, 7)] , \n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "# Find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=3, scoring='f1_macro')\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "grid_search.fit(df_train_sample['name'], df_train_sample['sex'])\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(names, genders, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=True,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 228.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='char_wb', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.5,\n",
       "                                 max_features=20000, min_df=0.0,\n",
       "                                 ngram_range=(2, 7), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=0, solver='lbfgs', tol=0.0001,\n",
       "                                    verbose=True, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2 = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(analyzer='char_wb', max_features=20000, ngram_range=(2,7), max_df=0.5, min_df=0.0)),\n",
    "        (\"tfidf\", TfidfTransformer(norm='l2')),\n",
    "        (\"clf\", LogisticRegression(random_state=0, verbose=True, n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1  0.9868154401 0.9830110438 0.9849095682   2726948\n",
      "           2  0.9798914883 0.9843833596 0.9821322880   2293387\n",
      "\n",
      "    accuracy                      0.9836379445   5020335\n",
      "   macro avg  0.9833534642 0.9836972017 0.9835209281   5020335\n",
      "weighted avg  0.9836524438 0.9836379445 0.9836408524   5020335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, digits=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VkGenderLogit.joblib']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(pipeline2, 'VkGenderLogit.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('VkGenderLogit.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1  0.9868154401 0.9830110438 0.9849095682   2726948\n",
      "           2  0.9798914883 0.9843833596 0.9821322880   2293387\n",
      "\n",
      "    accuracy                      0.9836379445   5020335\n",
      "   macro avg  0.9833534642 0.9836972017 0.9835209281   5020335\n",
      "weighted avg  0.9836524438 0.9836379445 0.9836408524   5020335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions2, digits=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
